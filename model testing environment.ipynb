{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9qedoaYktHN"
      },
      "source": [
        "## Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19bCNOm2c3i2",
        "outputId": "d8d7d6f2-3411-4efe-c5c9-3cb8403d9fae"
      },
      "outputs": [],
      "source": [
        "!pip install numpy\n",
        "!pip install opencv-python\n",
        "!pip install scipy\n",
        "!pip install wandb --quiet\n",
        "!pip install torch==1.12.1 torchvision==0.13.1 torchaudio==0.12.1\n",
        "!pip install torchsummary -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff34zOZd4VFd"
      },
      "source": [
        "## Kaggle Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPRXbaZg4Unp",
        "outputId": "90afd836-5712-4021-b84f-30a02870b161"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8 -q\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"KAGGLE_USERNAME\",\"key\":\"KAGGLE_KEY\"}') # TODO: Put your kaggle username & key here\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYrRMkvi4o4S",
        "outputId": "e4b4ff6e-d21f-4b86-fad4-60af982e4c71"
      },
      "outputs": [],
      "source": [
        "# Download the dataset with dalle and midjourney art as well as human art\n",
        "!kaggle datasets download -d \"superpotato9/dalle-recognition-dataset\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPi-CUu26H0o",
        "outputId": "a8738c45-d714-4eec-9d33-0c1ce1cc6647"
      },
      "outputs": [],
      "source": [
        "# unzip data\n",
        "!unzip -q dalle-recognition-dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_fdlByVTkz7p"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tzr9ll32c0Zz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "from scipy.ndimage import rotate\n",
        "\n",
        "import PIL.Image\n",
        "PIL.Image.MAX_IMAGE_PIXELS = None\n",
        "import random\n",
        "import concurrent.futures\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "import os\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.io import read_image\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unzJmOHmoADV"
      },
      "source": [
        "## Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBaSpJgroCKE"
      },
      "outputs": [],
      "source": [
        "config = { # TODO: Set your configuration values\n",
        "    \"lr\"         : 2e-3,\n",
        "    \"epochs\"     : 30,\n",
        "    \"batch_size\" : 32,\n",
        "    \"crop_size\" : 32, #size of image patches\n",
        "    \"num_crops\" : 192, #number of crops made from the image\n",
        "    \"crops_used\" : 64, #number of crops used to make the patchwork image to run through the model\n",
        "    \"use_subtract\" : True, #decides if we will subtract different texture images before or concat them to put through the model\n",
        "    \"keep_medium_texture\" : False,\n",
        "    \"model\" : \"resnet\", # can be 'resnet', 'inception', or 'efficientnet'\n",
        "    \"filters\" : [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"], #can have any combination of a,b,c,d,e,f,g,h. h is the garbor filters\n",
        "}\n",
        "config[\"nrow\"] = int(config[\"crops_used\"]**0.5)\n",
        "\n",
        "config[\"run_name\"] = \"Abl-size-{}-num-{}-subs-{}-med-{}-mod-{}-filters-{}\".format(\n",
        "    config[\"crop_size\"],\n",
        "    config[\"num_crops\"],\n",
        "    config[\"use_subtract\"],\n",
        "    config[\"keep_medium_texture\"],\n",
        "    config[\"model\"],\n",
        "    config[\"filters\"]\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OpcQyIYdiwK"
      },
      "source": [
        "## FILTERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Fyf2rW8diPR"
      },
      "outputs": [],
      "source": [
        "def apply_filter_a(src):\n",
        "    src_copy = np.copy(src)\n",
        "    f1 = np.array([[[ 0,  0,  0,  0,  0],\n",
        "        [ 0,  1,  0,  0,  0],\n",
        "        [ 0,  0, -1,  0,  0],\n",
        "        [ 0,  0,  0,  0,  0],\n",
        "        [ 0,  0,  0,  0,  0]],\n",
        "\n",
        "       [[ 0,  0,  0,  0,  0],\n",
        "        [ 0,  0,  1,  0,  0],\n",
        "        [ 0,  0, -1,  0,  0],\n",
        "        [ 0,  0,  0,  0,  0],\n",
        "        [ 0,  0,  0,  0,  0]],\n",
        "\n",
        "       [[ 0,  0,  0,  0,  0],\n",
        "        [ 0,  0,  0,  1,  0],\n",
        "        [ 0,  0, -1,  0,  0],\n",
        "        [ 0,  0,  0,  0,  0],\n",
        "        [ 0,  0,  0,  0,  0]],\n",
        "\n",
        "       [[ 0,  0,  0,  0,  0],\n",
        "        [ 0,  0,  0,  0,  0],\n",
        "        [ 0,  0, -1,  1,  0],\n",
        "        [ 0,  0,  0,  0,  0],\n",
        "        [ 0,  0,  0,  0,  0]],\n",
        "\n",
        "       [[ 0,  0,  0,  0,  0],\n",
        "        [ 0,  0,  0,  0,  0],\n",
        "        [ 0,  0, -1,  0,  0],\n",
        "        [ 0,  0,  0,  1,  0],\n",
        "        [ 0,  0,  0,  0,  0]],\n",
        "\n",
        "       [[ 0,  0,  0,  0,  0],\n",
        "        [ 0,  0,  0,  0,  0],\n",
        "        [ 0,  0, -1,  0,  0],\n",
        "        [ 0,  0,  1,  0,  0],\n",
        "        [ 0,  0,  0,  0,  0]],\n",
        "\n",
        "       [[ 0,  0,  0,  0,  0],\n",
        "        [ 0,  0,  0,  0,  0],\n",
        "        [ 0,  0, -1,  0,  0],\n",
        "        [ 0,  1,  0,  0,  0],\n",
        "        [ 0,  0,  0,  0,  0]],\n",
        "\n",
        "       [[ 0,  0,  0,  0,  0],\n",
        "        [ 0,  0,  0,  0,  0],\n",
        "        [ 0,  1, -1,  0,  0],\n",
        "        [ 0,  0,  0,  0,  0],\n",
        "        [ 0,  0,  0,  0,  0]]])\n",
        "\n",
        "    img = cv2.filter2D(src=src_copy, kernel=f1[0], ddepth=-1)\n",
        "    for filter in f1[1:]:\n",
        "        img = cv2.add(img,cv2.filter2D(src=src_copy, kernel=filter, ddepth=-1))\n",
        "\n",
        "    return img//8\n",
        "\n",
        "def apply_filter_b(src):\n",
        "    src_copy = np.copy(src)\n",
        "    f2 = np.array([[[ 0,  0,  0,  0,  0],\n",
        "                    [ 0,  2,  1,  0,  0],\n",
        "                    [ 0,  1, -3,  0,  0],\n",
        "                    [ 0,  0,  0,  1,  0],\n",
        "                    [ 0,  0,  0,  0,  0]],\n",
        "\n",
        "                    [[ 0,  0, -1,  0,  0],\n",
        "                    [ 0,  0,  3,  0,  0],\n",
        "                    [ 0,  0, -3,  0,  0],\n",
        "                    [ 0,  0,  1,  0,  0],\n",
        "                    [ 0,  0,  0,  0,  0]],\n",
        "\n",
        "                    [[ 0,  0,  0,  0,  0],\n",
        "                    [ 0,  0,  1,  2,  0],\n",
        "                    [ 0,  0, -3,  1,  0],\n",
        "                    [ 0,  1,  0,  0,  0],\n",
        "                    [ 0,  0,  0,  0,  0]],\n",
        "\n",
        "                    [[ 0,  0,  0,  0,  0],\n",
        "                    [ 0,  0,  0,  0,  0],\n",
        "                    [ 0,  1, -3,  3, -1],\n",
        "                    [ 0,  0,  0,  0,  0],\n",
        "                    [ 0,  0,  0,  0,  0]],\n",
        "\n",
        "                    [[ 0,  0,  0,  0,  0],\n",
        "                    [ 0,  1,  0,  0,  0],\n",
        "                    [ 0,  0, -3,  1,  0],\n",
        "                    [ 0,  0,  1,  2,  0],\n",
        "                    [ 0,  0,  0,  0,  0]],\n",
        "\n",
        "                    [[ 0,  0,  0,  0,  0],\n",
        "                    [ 0,  0,  1,  0,  0],\n",
        "                    [ 0,  0, -3,  0,  0],\n",
        "                    [ 0,  0,  3,  0,  0],\n",
        "                    [ 0,  0, -1,  0,  0]],\n",
        "\n",
        "                    [[ 0,  0,  0,  0,  0],\n",
        "                    [ 0,  0,  0,  1,  0],\n",
        "                    [ 0,  1, -3,  0,  0],\n",
        "                    [ 0,  2,  1,  0,  0],\n",
        "                    [ 0,  0,  0,  0,  0]],\n",
        "\n",
        "                    [[ 0,  0,  0,  0,  0],\n",
        "                    [ 0,  0,  0,  0,  0],\n",
        "                    [-1,  3, -3,  1,  0],\n",
        "                    [ 0,  0,  0,  0,  0],\n",
        "                    [ 0,  0,  0,  0,  0]]])\n",
        "\n",
        "    img = cv2.filter2D(src=src_copy, kernel=f2[0], ddepth=-1)\n",
        "    for filter in f2[1:]:\n",
        "        img = cv2.add(img,cv2.filter2D(src=src_copy, kernel=filter, ddepth=-1))\n",
        "\n",
        "    return img//8\n",
        "\n",
        "\n",
        "\n",
        "def apply_filter_c(src):\n",
        "    src_copy=np.copy(src)\n",
        "    f3 = np.array([[[ 0,  0,  0,  0,  0],\n",
        "                    [ 0,  0,  1,  0,  0],\n",
        "                    [ 0,  0, -2,  0,  0],\n",
        "                    [ 0,  0,  1,  0,  0],\n",
        "                    [ 0,  0,  0,  0,  0]],\n",
        "\n",
        "                    [[ 0,  0,  0,  0,  0],\n",
        "                    [ 0,  0,  0,  0,  0],\n",
        "                    [ 0,  1, -2,  1,  0],\n",
        "                    [ 0,  0,  0,  0,  0],\n",
        "                    [ 0,  0,  0,  0,  0]],\n",
        "\n",
        "                    [[ 0,  0,  0,  0,  0],\n",
        "                    [ 0,  1,  0,  0,  0],\n",
        "                    [ 0,  0, -2,  0,  0],\n",
        "                    [ 0,  0,  0,  1,  0],\n",
        "                    [ 0,  0,  0,  0,  0]],\n",
        "\n",
        "                    [[ 0,  0,  0,  0,  0],\n",
        "                    [ 0,  0,  0,  1,  0],\n",
        "                    [ 0,  0, -2,  0,  0],\n",
        "                    [ 0,  1,  0,  0,  0],\n",
        "                    [ 0,  0,  0,  0,  0]]])\n",
        "\n",
        "    img = cv2.filter2D(src=src_copy, kernel=f3[0], ddepth=-1)\n",
        "    for filter in f3[1:]:\n",
        "        img = cv2.add(img,cv2.filter2D(src=src_copy, kernel=filter, ddepth=-1))\n",
        "\n",
        "    return img//4\n",
        "\n",
        "\n",
        "def apply_filter_d(src):\n",
        "    src_copy=np.copy(src)\n",
        "    f4 = np.array([[[ 0,  0,  0,  0,  0],\n",
        "                    [ 0, -1,  2, -1,  0],\n",
        "                    [ 0,  2, -4,  2,  0],\n",
        "                    [ 0,  0,  0,  0,  0],\n",
        "                    [ 0,  0,  0,  0,  0]],\n",
        "\n",
        "                    [[ 0,  0,  0,  0,  0],\n",
        "                    [ 0, -1,  2,  0,  0],\n",
        "                    [ 0,  2, -4,  0,  0],\n",
        "                    [ 0, -1,  2,  0,  0],\n",
        "                    [ 0,  0,  0,  0,  0]],\n",
        "\n",
        "                    [[ 0,  0,  0,  0,  0],\n",
        "                    [ 0,  0,  0,  0,  0],\n",
        "                    [ 0,  2, -4,  2,  0],\n",
        "                    [ 0, -1,  2, -1,  0],\n",
        "                    [ 0,  0,  0,  0,  0]],\n",
        "\n",
        "                    [[ 0,  0,  0,  0,  0],\n",
        "                    [ 0,  0,  2, -1,  0],\n",
        "                    [ 0,  0, -4,  2,  0],\n",
        "                    [ 0,  0,  2, -1,  0],\n",
        "                    [ 0,  0,  0,  0,  0]]])\n",
        "\n",
        "    img = cv2.filter2D(src=src_copy, kernel=f4[0], ddepth=-1)\n",
        "    for filter in f4[1:]:\n",
        "        img = cv2.add(img,cv2.filter2D(src=src_copy, kernel=filter, ddepth=-1))\n",
        "\n",
        "    return img//4\n",
        "\n",
        "def apply_filter_e(src):\n",
        "    src_copy=np.copy(src)\n",
        "    f5 = np.array([[[  1,   2,  -2,   2,   1],\n",
        "                    [  2,  -6,   8,  -6,   2],\n",
        "                    [ -2,   8, -12,   8,  -2],\n",
        "                    [  0,   0,   0,   0,   0],\n",
        "                    [  0,   0,   0,   0,   0]],\n",
        "\n",
        "                [[  1,   2,  -2,   0,   0],\n",
        "                    [  2,  -6,   8,   0,   0],\n",
        "                    [ -2,   8, -12,   0,   0],\n",
        "                    [  2,  -6,   8,   0,   0],\n",
        "                    [  1,   2,  -2,   0,   0]],\n",
        "\n",
        "                [[  0,   0,   0,   0,   0],\n",
        "                    [  0,   0,   0,   0,   0],\n",
        "                    [ -2,   8, -12,   8,  -2],\n",
        "                    [  2,  -6,   8,  -6,   2],\n",
        "                    [  1,   2,  -2,   2,   1]],\n",
        "\n",
        "                [[  0,   0,  -2,   2,   1],\n",
        "                    [  0,   0,   8,  -6,   2],\n",
        "                    [  0,   0, -12,   8,  -2],\n",
        "                    [  0,   0,   8,  -6,   2],\n",
        "                    [  0,   0,  -2,   2,   1]]])\n",
        "\n",
        "    img = cv2.filter2D(src=src_copy, kernel=f5[0], ddepth=-1)\n",
        "    for filter in f5[1:]:\n",
        "        img=cv2.add(img,cv2.filter2D(src=src_copy, kernel=filter, ddepth=-1))\n",
        "\n",
        "    return img//4\n",
        "\n",
        "def apply_filter_f(src):\n",
        "    src_copy=np.copy(src)\n",
        "    f5 = np.asarray([[ 0,  0,  0,  0,  0],\n",
        "                    [ 0,  -1,  2, -1,  0],\n",
        "                    [ 0,  2,  -4,  2,  0],\n",
        "                    [ 0,  -1,  2, -1,  0],\n",
        "                    [ 0,  0,  0,  0,  0]])\n",
        "\n",
        "    img = cv2.filter2D(src=src_copy, kernel=f5, ddepth=-1)\n",
        "    return img\n",
        "\n",
        "\n",
        "def apply_filter_g(src):\n",
        "    src_copy=np.copy(src)\n",
        "    f5 = np.asarray([[ -1,   2,  -2,   2,  -1],\n",
        "                    [  2,  -6,   8,  -6,   2],\n",
        "                    [ -2,   8, -12,   8,  -2],\n",
        "                    [  2,  -6,   8,  -6,   2],\n",
        "                    [ -1,   2,  -2,   2,  -1]])\n",
        "\n",
        "    img = cv2.filter2D(src=src_copy, kernel=f5, ddepth=-1)\n",
        "    return img\n",
        "\n",
        "def build_gabor_filters(ksize=31, sigma=4.0, lambd=10.0, gamma=0.5, psi=0, num_theta=4):\n",
        "    filters = []\n",
        "    for theta in np.linspace(0, np.pi, num_theta):  # Varying orientation from 0 to 180 degrees\n",
        "        kern = cv2.getGaborKernel((ksize, ksize), sigma, theta, lambd, gamma, psi, ktype=cv2.CV_32F)\n",
        "        filters.append(kern)\n",
        "    return filters\n",
        "\n",
        "def apply_gabor_filters(src, filters):\n",
        "    accum = np.zeros_like(src)\n",
        "    for kern in filters:\n",
        "        fimg = cv2.filter2D(src, cv2.CV_8UC3, kern)\n",
        "        np.maximum(accum, fimg, accum)  # Accumulate the maximum response\n",
        "    return accum\n",
        "\n",
        "def apply_all_filters(src):\n",
        "    gab_fil = build_gabor_filters(ksize=20, sigma=4, lambd=10, gamma=0.5, psi=2, num_theta=2)\n",
        "\n",
        "    src_copy = np.copy(src)\n",
        "    filDict = {}\n",
        "    filDict[\"a\"] = apply_filter_a(src_copy)\n",
        "    filDict[\"b\"] = apply_filter_b(src_copy)\n",
        "    filDict[\"c\"] = apply_filter_c(src_copy)\n",
        "    filDict[\"d\"] = apply_filter_d(src_copy)\n",
        "    filDict[\"e\"] = apply_filter_e(src_copy)\n",
        "    filDict[\"f\"] = apply_filter_f(src_copy)\n",
        "    filDict[\"g\"] = apply_filter_g(src_copy)\n",
        "    filDict[\"h\"] = apply_gabor_filters(src_copy, gab_fil)\n",
        "    if len(config[\"filters\"]) == 0:\n",
        "      filters = src_copy\n",
        "      return (np.array(cv2.cvtColor(filters, cv2.COLOR_RGB2GRAY))).astype(np.float64).reshape((1, filters.shape[0], filters.shape[1]))\n",
        "    f = filDict[\"a\"]+filDict[\"b\"]\n",
        "    filters = filDict[config[\"filters\"][0]]\n",
        "    for i in range(1,len(config[\"filters\"])):\n",
        "      filters += filDict[config[\"filters\"][i]]\n",
        "\n",
        "    num_fil = len(config[\"filters\"])\n",
        "    filters = filters.reshape((filters.shape[1], filters.shape[2], -1))\n",
        "    return (np.array(cv2.cvtColor(filters, cv2.COLOR_RGB2GRAY))//num_fil).astype(np.float64).reshape((1, filters.shape[0], filters.shape[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0wWq_DhsSu5"
      },
      "source": [
        "## Patch Gen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrN4oL-VsZhX"
      },
      "outputs": [],
      "source": [
        "def calculate_l1(patch):\n",
        "    # Assuming patch size of (3, 8, 8)\n",
        "    patch = patch.sum(dim=0)  # Sum over channels\n",
        "\n",
        "    # Calculate differences between adjacent pixels\n",
        "    diff_hor = torch.abs(patch[:, 1:] - patch[:, :-1]).sum()\n",
        "    diff_ver = torch.abs(patch[1:, :] - patch[:-1, :]).sum()\n",
        "\n",
        "    # Diagonal differences\n",
        "    diff_diag1 = torch.abs(patch[1:, 1:] - patch[:-1, :-1]).sum()\n",
        "    diff_diag2 = torch.abs(patch[:-1, 1:] - patch[1:, :-1]).sum()\n",
        "\n",
        "    # Total L1 norm\n",
        "    total_l1 = diff_hor + diff_ver + diff_diag1 + diff_diag2\n",
        "    return total_l1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41RLZYDKscn4"
      },
      "outputs": [],
      "source": [
        "def return_rich_poor_images(image, crop_size, num_crops, crops_used, include_mediums):\n",
        "# Define the transform\n",
        "    transform = transforms.RandomCrop(crop_size)  # Define the crop size\n",
        "\n",
        "    crops = [transform(image) for _ in range(num_crops)]\n",
        "    diversities = [calculate_l1(crop) for crop in crops]\n",
        "    paired_crops_diversities = list(zip(crops, diversities))\n",
        "    paired_crops_diversities.sort(key=lambda x: x[1])\n",
        "\n",
        "    sorted_crops, sorted_diversities = zip(*paired_crops_diversities)\n",
        "\n",
        "    sorted_crops = list(sorted_crops)\n",
        "    sorted_diversities = list(sorted_diversities)\n",
        "\n",
        "    poor_pixels = sorted_crops[:crops_used]\n",
        "    rich_pixels = sorted_crops[-crops_used:]\n",
        "\n",
        "    poor_tensor = torch.stack(poor_pixels)\n",
        "    poor_grid = make_grid(poor_tensor, nrow=config[\"nrow\"], padding=0)\n",
        "    poor_grid = apply_all_filters(poor_grid)\n",
        "\n",
        "    rich_tensor = torch.stack(rich_pixels)\n",
        "    rich_grid = make_grid(rich_tensor, nrow=config[\"nrow\"], padding=0)\n",
        "    rich_grid = apply_all_filters(rich_grid)\n",
        "\n",
        "    if include_mediums:\n",
        "      halfway = num_crops // 2\n",
        "      medium_pixels = sorted_crops[halfway-(crops_used//2):halfway+(crops_used//2)]\n",
        "      medium_tensor = torch.stack(poor_pixels)\n",
        "      medium_grid = make_grid(poor_tensor, nrow=config[\"nrow\"], padding=0)\n",
        "      medium_grid = apply_all_filters(medium_grid)\n",
        "      return poor_grid, rich_grid, medium_grid\n",
        "\n",
        "    return poor_grid, rich_grid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqCTDEv9VOxn"
      },
      "source": [
        "## Data and Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdbN2HdoVOx2"
      },
      "outputs": [],
      "source": [
        "class MakeDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, img_paths, labels):\n",
        "    self.labels = []\n",
        "    self.imgs = []\n",
        "\n",
        "    for i in range(len(img_paths)):\n",
        "      image = read_image(img_paths[i])\n",
        "      if image.shape[0] == 3:\n",
        "        self.imgs.append(image)\n",
        "        self.labels.append(labels[i])\n",
        "\n",
        "    self.length = len(self.labels)\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "  def __getitem__(self, ind):\n",
        "    image = self.imgs[ind]\n",
        "    texture_regions = return_rich_poor_images(image, config['crop_size'], config['num_crops'], config['crops_used'], config['keep_medium_texture'])\n",
        "    return texture_regions, torch.tensor(self.labels[ind])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jbju8r_VOx2",
        "outputId": "a97f89be-f68c-4bdd-c4f9-c03fe0390a44"
      },
      "outputs": [],
      "source": [
        "train_dir_ai = '/content/fakeV2/fake-v2'\n",
        "train_dir_real = '/content/real'\n",
        "\n",
        "ai_imgs = []\n",
        "for img in os.listdir(train_dir_ai):\n",
        "  if img[-3:] == \"jpg\":\n",
        "    ai_imgs.append(os.path.join(train_dir_ai,img))\n",
        "\n",
        "real_imgs = []\n",
        "for img in os.listdir(train_dir_real):\n",
        "  if img[-3:] == \"jpg\":\n",
        "    real_imgs.append(os.path.join(train_dir_real,img))\n",
        "\n",
        "ai_label = [1 for i in range(len(ai_imgs))]\n",
        "real_label = [0 for i in range(len(real_imgs))]\n",
        "\n",
        "shorterLen = len(ai_imgs)\n",
        "if len(real_imgs) < shorterLen:\n",
        "  shorterLen = len(real_imgs)\n",
        "\n",
        "# shorterLen = 100\n",
        "train_split = list(np.arange(shorterLen))\n",
        "random.shuffle(train_split)\n",
        "X_train = [ai_imgs[i] for i in train_split[:int(shorterLen*0.7)]] + [real_imgs[i] for i in train_split[:int(shorterLen*0.7)]]\n",
        "y_train = [ai_label[i] for i in train_split[:int(shorterLen*0.7)]] + [real_label[i] for i in train_split[:int(shorterLen*0.7)]]\n",
        "X_validate = [ai_imgs[i] for i in train_split[int(shorterLen*0.7):]] + [real_imgs[i] for i in train_split[int(shorterLen*0.7):]]\n",
        "y_validate = [ai_label[i] for i in train_split[int(shorterLen*0.7):]] + [real_label[i] for i in train_split[int(shorterLen*0.7):]]\n",
        "print(len(X_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhV6He7yVOx2"
      },
      "outputs": [],
      "source": [
        "train_data = MakeDataset(X_train, y_train)\n",
        "val_data = MakeDataset(X_validate, y_validate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7SvqvkJVOx2"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = train_data,\n",
        "    num_workers = 4,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = True\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = val_data,\n",
        "    num_workers = 2,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX0IOBzmVOx3",
        "outputId": "f515e23f-66cd-4f0e-cef4-0bb9f63ee24e"
      },
      "outputs": [],
      "source": [
        "# sanity check\n",
        "for data in train_loader:\n",
        "    x, y = data\n",
        "    print(len(x), y.shape)\n",
        "    print(x[0].shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-fJFjdmVEc3"
      },
      "source": [
        "## MODEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjovKq22OO-q"
      },
      "source": [
        "### Fingerprinting Stem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ibmByXOVEdA"
      },
      "outputs": [],
      "source": [
        "class SplitConv(nn.Module):\n",
        "    def __init__(self, use_subtract, keep_medium_texture, in_channels=3):\n",
        "        super(SplitConv, self).__init__()\n",
        "\n",
        "        self.use_subtract = use_subtract\n",
        "        self.keep_medium_texture = keep_medium_texture\n",
        "        out = 32\n",
        "        inChan = in_channels\n",
        "        self.block = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(inChan, out, 3, 1, 0),\n",
        "            torch.nn.BatchNorm2d(out),\n",
        "            torch.nn.Hardtanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        rich = self.block(x[0].to(device=DEVICE, dtype=torch.float))\n",
        "        poor = self.block(x[1].to(device=DEVICE, dtype=torch.float))\n",
        "        rich_poor = torch.subtract(rich,poor) if self.use_subtract else torch.concat((rich, poor), dim=1)\n",
        "        if self.keep_medium_texture:\n",
        "          medium = self.block(x[2].to(device=DEVICE, dtype=torch.float))\n",
        "          return torch.concat((rich_poor, medium), dim=1)\n",
        "        else:\n",
        "          return rich_poor\n",
        "\n",
        "class Stem(nn.Module):\n",
        "    def __init__(self, use_subtract=config['use_subtract'], keep_medium_texture=config['keep_medium_texture'], in_channels=3, stem_output_channels=32):\n",
        "        super(Stem, self).__init__()\n",
        "        self.backbone = torch.nn.Sequential(\n",
        "                          SplitConv(use_subtract, keep_medium_texture, in_channels=in_channels))\n",
        "        self.temp = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(stem_output_channels, stem_output_channels, 3, 1, 0),\n",
        "        )\n",
        "        self.block = torch.nn.Sequential(\n",
        "\n",
        "                          torch.nn.BatchNorm2d(stem_output_channels),\n",
        "                          torch.nn.ReLU(),\n",
        "                          torch.nn.Conv2d(stem_output_channels, stem_output_channels, 3, 1, 0),\n",
        "                          torch.nn.BatchNorm2d(stem_output_channels),\n",
        "                          torch.nn.ReLU(),\n",
        "                          torch.nn.Conv2d(stem_output_channels, stem_output_channels, 3, 1, 0),\n",
        "                          torch.nn.BatchNorm2d(stem_output_channels),\n",
        "                          torch.nn.ReLU(),\n",
        "                          torch.nn.Conv2d(stem_output_channels, stem_output_channels, 3, 1, 0),\n",
        "                          torch.nn.BatchNorm2d(stem_output_channels),\n",
        "                          torch.nn.ReLU())\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.backbone(x)\n",
        "      x = self.temp(x)\n",
        "      return self.block(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dz0yEleYNIz7"
      },
      "source": [
        "### Inceptionv1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2N_o_s5_l-6F"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "            torch.nn.BatchNorm2d(out_channels),\n",
        "            torch.nn.ReLU()\n",
        "            )\n",
        "\n",
        "    def forward(self, x, return_feats=False):\n",
        "\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9PYgwVEt6yF"
      },
      "outputs": [],
      "source": [
        "class InceptionBlock(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, block_1x1, block_3x3_in, block_3x3_out, block_5x5_in, block_5x5_out, block_pool_out):\n",
        "    super().__init__()\n",
        "\n",
        "    self.block_1x1 = ConvBlock(in_channels, block_1x1, kernel_size=1, padding=0, stride=1)\n",
        "\n",
        "    self.block_3x3 = torch.nn.Sequential(\n",
        "        ConvBlock(in_channels, block_3x3_in, kernel_size=1, padding=0, stride=1),\n",
        "        ConvBlock(block_3x3_in, block_3x3_out, kernel_size=3, padding=1, stride=1),\n",
        "        )\n",
        "\n",
        "    self.block_5x5 = torch.nn.Sequential(\n",
        "        ConvBlock(in_channels, block_5x5_in, kernel_size=1, padding=0, stride=1),\n",
        "        ConvBlock(block_5x5_in, block_5x5_out, kernel_size=5, padding=2, stride=1),\n",
        "        )\n",
        "\n",
        "    self.block_pool = torch.nn.Sequential(\n",
        "        torch.nn.MaxPool2d(kernel_size=3, padding=1, stride=1),\n",
        "        ConvBlock(in_channels, block_pool_out, kernel_size=1, padding=0, stride=1),\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    block_1x1_out = self.block_1x1(x)\n",
        "    block_3x3_out = self.block_3x3(x)\n",
        "    block_5x5_out = self.block_5x5(x)\n",
        "    block_pool_out = self.block_pool(x)\n",
        "    return torch.cat([block_1x1_out, block_3x3_out, block_5x5_out, block_pool_out], 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXW7w55vuLXE"
      },
      "outputs": [],
      "source": [
        "class InceptionModelv1(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels=32, num_classes=2):\n",
        "    super().__init__()\n",
        "\n",
        "    self.backbone = torch.nn.Sequential(\n",
        "        # section 1\n",
        "        ConvBlock(in_channels, 64, kernel_size=7, padding=3, stride=2),\n",
        "        torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "        # section 2\n",
        "        ConvBlock(64, 192, kernel_size=3, padding=1, stride=1),\n",
        "        torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "        # section 3\n",
        "        InceptionBlock(192, 64, 96, 128, 16, 32, 32),\n",
        "        InceptionBlock(256, 128, 128, 192, 32, 96, 64),\n",
        "        torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "        # Section 4\n",
        "        InceptionBlock(480, 192, 96, 208, 16, 48, 64),\n",
        "        InceptionBlock(512, 160, 112, 224, 24, 64, 64),\n",
        "        InceptionBlock(512, 128, 128, 256, 24, 64, 64),\n",
        "        InceptionBlock(512, 112, 144, 288, 32, 64, 64),\n",
        "        InceptionBlock(528, 256, 160, 320, 32, 128, 128),\n",
        "        torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "        # Section 5\n",
        "        InceptionBlock(832, 256, 160, 320, 32, 128, 128),\n",
        "        InceptionBlock(832, 384, 192, 384, 48, 128, 128),\n",
        "        torch.nn.AvgPool2d(kernel_size=8, stride=1),\n",
        "        torch.nn.Dropout(p=0.4),\n",
        "        torch.nn.Flatten(),\n",
        "        )\n",
        "\n",
        "    self.cls_layer = torch.nn.Linear(1024, num_classes)\n",
        "\n",
        "  def forward(self, x, return_feats=False):\n",
        "    feats = self.backbone(x)\n",
        "    out = self.cls_layer(feats)\n",
        "\n",
        "    if return_feats:\n",
        "        return feats\n",
        "    else:\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRw6Jx97SoLX"
      },
      "source": [
        "### EfficientNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_kmklCoTg1t"
      },
      "source": [
        "Inspired by\n",
        "\n",
        "https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/Pytorch/CNN_architectures/pytorch_efficientnet.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W8shYOiOtsA"
      },
      "source": [
        "#### Conv Block\n",
        "packages conv with batchnorm and relu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yo5sSOUfOtsA"
      },
      "outputs": [],
      "source": [
        "class EnetConvBlock(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding, stride=1, depthwise=False):\n",
        "        super().__init__()\n",
        "\n",
        "        groups = in_channels if depthwise else 1\n",
        "\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, groups=groups),\n",
        "            torch.nn.BatchNorm2d(out_channels),\n",
        "            torch.nn.SiLU()\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        return self.layers(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BXp--5ROtsA"
      },
      "source": [
        "#### Squeeze Excitation\n",
        "Determine how important each layer is"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJJIorSlOtsB"
      },
      "outputs": [],
      "source": [
        "class SqueezeExcitation(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, upsized, squeezed):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.AdaptiveAvgPool2d((1, 1)), # pool across layer\n",
        "            torch.nn.Conv2d(upsized, squeezed, 1), # downsize number of layers\n",
        "            torch.nn.SiLU(), # activation\n",
        "            torch.nn.Conv2d(squeezed, upsized, 1), # upsize to input number\n",
        "            torch.nn.Sigmoid() # get importance of each layer as (0,1)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * self.layers(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCa1Z9YQOtsB"
      },
      "source": [
        "#### MBB Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cn4wYH3FOtsB"
      },
      "outputs": [],
      "source": [
        "class MBBlock(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride, expand_ratio, reduction):\n",
        "    super().__init__()\n",
        "    self.keep_prob = 0.8 # odds of not skipping the block\n",
        "    self.use_residual = (in_channels == out_channels) and (stride == 1)\n",
        "    self.stride = stride\n",
        "    upsized = int(in_channels * expand_ratio) # increased for inverted bottlenck\n",
        "    self.expand_ratio = expand_ratio\n",
        "    squeezed = int(in_channels / reduction) # squeezed size in squeezeExcitation\n",
        "\n",
        "    self.expand = EnetConvBlock(in_channels, upsized, 1, 0, 1) # upsize the input\n",
        "\n",
        "    self.bottleneck = torch.nn.Sequential(\n",
        "        EnetConvBlock(upsized, upsized, kernel_size, kernel_size//2, stride, True), # do the depthwise conv\n",
        "        SqueezeExcitation(upsized, squeezed),\n",
        "        torch.nn.Conv2d(upsized, out_channels, 1, bias=False),\n",
        "        torch.nn.BatchNorm2d(out_channels)\n",
        "    )\n",
        "\n",
        "  def randomlyDrop(self, x):\n",
        "    mask = torch.rand(x.shape[0], 1, 1, 1, device=x.device) < self.keep_prob\n",
        "    return (x/self.keep_prob) * mask\n",
        "\n",
        "  def forward(self, x):\n",
        "    # expand if necessary\n",
        "    if self.expand_ratio != 1:\n",
        "      expanded = self.expand(x)\n",
        "    else:\n",
        "      expanded = x\n",
        "\n",
        "    # do the bottleneck forward pass\n",
        "    output = self.bottleneck(expanded)\n",
        "    if not self.use_residual:\n",
        "      return output\n",
        "\n",
        "    # stochastic depth\n",
        "    if self.training:\n",
        "      output = self.randomlyDrop(output)\n",
        "    # return with residual\n",
        "    return x + output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaZ5HrWTOtsB"
      },
      "source": [
        "#### EfficientNet Network Itself"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny-mh_ocWIJR"
      },
      "outputs": [],
      "source": [
        "from math import ceil\n",
        "class EfficientNet(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, in_channels=32, num_classes=2):\n",
        "    super().__init__()\n",
        "    # get the base model values from the paper\n",
        "    base_model = [\n",
        "      # expand_ratio, channels, num_blocks, stride, kernel_size\n",
        "      (1, 16, 1, 1, 3),\n",
        "      (6, 24, 2, 2, 3),\n",
        "      (6, 40, 2, 2, 5),\n",
        "      (6, 80, 3, 2, 3),\n",
        "      (6, 112, 3, 1, 5),\n",
        "      (6, 192, 4, 2, 5),\n",
        "      (6, 320, 1, 1, 3),\n",
        "    ]\n",
        "\n",
        "    # calculate hyperpamaters for network\n",
        "    phi = 1.08 # change to scale network\n",
        "    depth_factor = 1.2 ** phi\n",
        "    width_factor = 1.1 ** phi\n",
        "    dropout = 0.5\n",
        "    reduction = 4 # how much to squeeze in squeezeExcite\n",
        "\n",
        "    embedding_size = ceil(1280 * width_factor)\n",
        "\n",
        "    # add the initial conv layer layers\n",
        "    layers = []\n",
        "    inital_size = in_channels\n",
        "    in_channels = int(32 * width_factor)\n",
        "    layers.append(EnetConvBlock(inital_size, in_channels, 3, 1, 2)) # initial image processing\n",
        "\n",
        "    # add all the bottleneck layers\n",
        "    for (expand_ratio, channels, num_blocks, stride, kernel_size) in base_model:\n",
        "      out_channels = reduction * ceil(int(channels * width_factor) / reduction) # make number div by 4 for squeezeExcitation\n",
        "      for block_num in range(ceil(num_blocks * depth_factor)):\n",
        "        if block_num == 0:\n",
        "          layers.append(MBBlock(in_channels, out_channels, kernel_size, stride, expand_ratio, reduction)) # downsampling block\n",
        "        else:\n",
        "          layers.append(MBBlock(in_channels, out_channels, kernel_size, 1, expand_ratio, reduction)) # just regular conv block\n",
        "        in_channels = out_channels\n",
        "\n",
        "    # add the classifier stuff\n",
        "    layers.append(EnetConvBlock(in_channels, embedding_size, 1, 0, 1)) # 1x1 conv to final size\n",
        "    layers.append(torch.nn.AdaptiveAvgPool2d((1,1)))\n",
        "    layers.append(torch.nn.Flatten())\n",
        "    layers.append(torch.nn.Dropout(p=dropout))\n",
        "    self.backbone = torch.nn.Sequential(*layers)\n",
        "\n",
        "    self.cls_layer = torch.nn.Linear(embedding_size, num_classes)\n",
        "\n",
        "  def forward(self, x, return_feats=False):\n",
        "    feats = self.backbone(x)\n",
        "    out = self.cls_layer(feats)\n",
        "\n",
        "    if return_feats:\n",
        "        return feats\n",
        "    else:\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiyTWO_7VEdC"
      },
      "source": [
        "### Resnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ci_plbiuVEdC"
      },
      "outputs": [],
      "source": [
        "class basicConvBlock(torch.nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, activation_function=True):\n",
        "        super(basicConvBlock, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
        "                            stride=stride, padding=padding, bias=False))\n",
        "        layers.append(torch.nn.BatchNorm2d(out_channels))\n",
        "        if activation_function:\n",
        "          layers.append(torch.nn.SiLU())\n",
        "        self.model = torch.nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaWaPvH5VEdC"
      },
      "outputs": [],
      "source": [
        "class ResNetBasicBlock(torch.nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, downsample_block=None):\n",
        "    super(ResNetBasicBlock, self).__init__()\n",
        "    self.downsample_block = downsample_block\n",
        "    self.model = torch.nn.Sequential(\n",
        "        basicConvBlock(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "        basicConvBlock(in_channels=out_channels, out_channels=out_channels, kernel_size=kernel_size,\n",
        "                       stride=1, padding=padding, activation_function=None)\n",
        "    )\n",
        "    self.activation_function = torch.nn.SiLU()\n",
        "  def forward(self, x):\n",
        "    y = self.model(x)\n",
        "    if self.downsample_block:\n",
        "      y += self.downsample_block(x)\n",
        "    else:\n",
        "      y += x\n",
        "    return self.activation_function(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMqCSyAdVEdC"
      },
      "outputs": [],
      "source": [
        "class ResNetModel(torch.nn.Module):\n",
        "  def __init__(self, in_channels=32, num_classes=2):\n",
        "    super(ResNetModel, self).__init__()\n",
        "    self.in_channels = in_channels*2\n",
        "    layers = []\n",
        "    layers.append(basicConvBlock(in_channels=in_channels, out_channels=self.in_channels,\n",
        "                                 kernel_size=7, stride=2, padding=3,\n",
        "                                 activation_function=True))\n",
        "    layers.append(torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
        "\n",
        "    configs = [\n",
        "        [self.in_channels, 2, 1],\n",
        "        [self.in_channels*2, 2, 2],\n",
        "        [self.in_channels*4, 2, 2],\n",
        "        [self.in_channels*8, 2, 2]\n",
        "    ]\n",
        "\n",
        "    self.cls_layer = torch.nn.Linear(self.in_channels*8, num_classes)\n",
        "    for out_channels, number, stride in configs:\n",
        "      self.build_blocks(out_channels, number, stride, layers=layers)\n",
        "    layers.append(torch.nn.AdaptiveAvgPool2d((1,1)))\n",
        "    layers.append(torch.nn.Flatten())\n",
        "    self.backbone = torch.nn.Sequential(*layers)\n",
        "\n",
        "  def build_blocks(self, out_channels, number, stride, layers):\n",
        "    downsample_block = None\n",
        "    if stride != 1:\n",
        "      downsample_block = basicConvBlock(in_channels=self.in_channels,\n",
        "                                        out_channels=out_channels,\n",
        "                                        kernel_size=1, stride=stride,\n",
        "                                        padding=0,\n",
        "                                        activation_function=False)\n",
        "    layers.append(ResNetBasicBlock(in_channels=self.in_channels,\n",
        "                                   out_channels=out_channels, stride=stride,\n",
        "                                   downsample_block=downsample_block))\n",
        "    self.in_channels = out_channels\n",
        "    for _ in range(1, number):\n",
        "      layers.append(ResNetBasicBlock(in_channels=self.in_channels,\n",
        "                                    out_channels=out_channels, stride=1))\n",
        "    return\n",
        "  def forward(self, x, return_feats=False):\n",
        "    feats = self.backbone(x)\n",
        "    out = self.cls_layer(feats)\n",
        "    if return_feats:\n",
        "        return feats\n",
        "    else:\n",
        "      return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX3tVLHwVEdC"
      },
      "source": [
        "### Model Init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfNdckgiIoTK"
      },
      "outputs": [],
      "source": [
        "# set classifier\n",
        "match config['model']:\n",
        "  case 'resnet':\n",
        "    classifier = ResNetModel\n",
        "  case 'inception':\n",
        "    classifier = InceptionModelv1\n",
        "  case 'efficientnet':\n",
        "    classifier = EfficientNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FT0GEYvJVEdC"
      },
      "outputs": [],
      "source": [
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "filter_output_channels = 1\n",
        "stem_output_channels = 32\n",
        "class_in_channels = stem_output_channels\n",
        "if not config['use_subtract']:\n",
        "  class_in_channels += stem_output_channels\n",
        "if config['keep_medium_texture']:\n",
        "  class_in_channels += stem_output_channels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fgwu87C463-O",
        "outputId": "8eafce78-90ca-4619-b542-fca3514920cf"
      },
      "outputs": [],
      "source": [
        "model = torch.nn.Sequential(Stem(in_channels=filter_output_channels, stem_output_channels=class_in_channels), classifier(in_channels=class_in_channels)).to(DEVICE)\n",
        "summary(model, (32,1, 256, 256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RGOwGoIhspP"
      },
      "source": [
        "## Loss, Optimizer, and Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8D-rWuWhr9N"
      },
      "outputs": [],
      "source": [
        "# Defining Loss function\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Defining Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# Defining Scheduler\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, config['epochs'])\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXLTtRoLidBu"
      },
      "source": [
        "## Train and Validate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZLHYg0cijxy"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJm9iamaillj"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, criterion):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Progress Bar\n",
        "    batch_bar   = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5)\n",
        "\n",
        "    num_correct = 0\n",
        "    total_loss  = 0\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "\n",
        "        optimizer.zero_grad() # Zero gradients\n",
        "\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        #with torch.cuda.amp.autocast(): # This implements mixed precision. Thats it!\n",
        "        outputs = model(images)\n",
        "        loss    = criterion(outputs, labels)\n",
        "\n",
        "        # Update no. of correct predictions & loss as we iterate\n",
        "        num_correct     += int((torch.argmax(outputs, axis=1) == labels).sum())\n",
        "        total_loss      += float(loss.item())\n",
        "\n",
        "        # tqdm lets you add some details so you can monitor training as you train.\n",
        "        batch_bar.set_postfix(\n",
        "            acc         = \"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n",
        "            loss        = \"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            num_correct = num_correct,\n",
        "            lr          = \"{:.04f}\".format(float(optimizer.param_groups[0]['lr']))\n",
        "        )\n",
        "\n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update()\n",
        "\n",
        "        batch_bar.update() # Update tqdm bar\n",
        "\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "    acc         = 100 * num_correct / (config['batch_size']* len(dataloader))\n",
        "    total_loss  = float(total_loss / len(dataloader))\n",
        "\n",
        "    return acc, total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01mgLHH0iw-p"
      },
      "source": [
        "### Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0MqmzEIizdN"
      },
      "outputs": [],
      "source": [
        "def validate(model, dataloader, criterion):\n",
        "\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val', ncols=5)\n",
        "\n",
        "    num_correct = 0.0\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "\n",
        "        # Move images to device\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        # Get model outputs\n",
        "        with torch.inference_mode():\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        num_correct += int((torch.argmax(outputs, axis=1) == labels).sum())\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            num_correct=num_correct)\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "    batch_bar.close()\n",
        "    acc = 100 * num_correct / (config['batch_size']* len(dataloader))\n",
        "    total_loss = float(total_loss / len(dataloader))\n",
        "    return acc, total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-kgHlK3j2hz"
      },
      "source": [
        "## WandB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWYBcz6vj5g9"
      },
      "outputs": [],
      "source": [
        "use_wandb = False # TODO: set True/False depending on if you are logging results to wandb\n",
        "if use_wandb:\n",
        "  wandb.login(key=\"YOUR_WANDB_KEY\") # TODO: Add your wandb key to log runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uivkZhfIj8V8"
      },
      "outputs": [],
      "source": [
        "run_name = \"Abl-size-{}-num-{}-subs-{}-med-{}-mod-{}\".format(\n",
        "    config[\"crop_size\"],\n",
        "    config[\"num_crops\"],\n",
        "    config[\"use_subtract\"],\n",
        "    config[\"keep_medium_texture\"],\n",
        "    config[\"model\"])\n",
        "\n",
        "if use_wandb:\n",
        "  run = wandb.init(\n",
        "      name = run_name, ## Wandb creates random run names if you skip this field\n",
        "      reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "      # id = \"n37w794d\", ### Insert specific run id here if you want to resume a previous run\n",
        "      # resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "      project = \"idl-project\", ### Project should be created in your wandb account\n",
        "      config = config ### Wandb Config for your run\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hkb6rRg9HE10"
      },
      "outputs": [],
      "source": [
        "if use_wandb:\n",
        "  model_arch  = str(model)\n",
        "\n",
        "  ### Save it in a txt file\n",
        "  arch_file   = open(\"model_arch.txt\", \"w\")\n",
        "  file_write  = arch_file.write(model_arch)\n",
        "  arch_file.close()\n",
        "\n",
        "  ### log it in your wandb run with wandb.save()\n",
        "  wandb.save('model_arch.txt')\n",
        "\n",
        "  ### Save parameters in a txt file\n",
        "  config_file   = open(\"config.txt\", \"w\")\n",
        "  file_write  = config_file.write(str(config))\n",
        "  config_file.close()\n",
        "\n",
        "  ### log it in your wandb run with wandb.save()\n",
        "  wandb.save('config.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTffABMJjN4L"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7YVGGhdIhNN"
      },
      "outputs": [],
      "source": [
        "best_val_acc        = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "nJMMZoDQjSNH",
        "outputId": "fc6768ad-52e3-43f7-82e8-fe9f4e2c57a9"
      },
      "outputs": [],
      "source": [
        "for epoch in range(config['epochs']):\n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    train_acc, train_loss = train(model, train_loader, optimizer, criterion)\n",
        "\n",
        "    print(\"\\nEpoch {}/{}: \\nTrain Acc {:.04f}%\\t Train Loss {:.04f}\\t Learning Rate {:.04f}\".format(\n",
        "        epoch + 1, config['epochs'], train_acc, train_loss, curr_lr))\n",
        "\n",
        "    val_acc, val_loss = validate(model, val_loader, criterion)\n",
        "    print(\"Val Acc {:.04f}%\\t Val Loss {:.04f}\".format(val_acc, val_loss))\n",
        "\n",
        "    if use_wandb:\n",
        "      wandb.log({\"train_acc\": train_acc,\n",
        "                \"train_loss\":train_loss,\n",
        "                \"val_acc\": val_acc,\n",
        "                \"val_loss\": val_loss,\n",
        "                \"learning_rate\": curr_lr})\n",
        "\n",
        "    if val_acc >= best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save({'model_state_dict':model.state_dict(),\n",
        "                    'optimizer_state_dict':optimizer.state_dict(),\n",
        "                    'scheduler_state_dict':scheduler.state_dict(),\n",
        "                    'val_acc': val_acc,\n",
        "                    'epoch': epoch}, './checkpoint_classification.pth')\n",
        "        if use_wandb:\n",
        "          wandb.save('checkpoint.pth')\n",
        "        print(\"Saved best model\")\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQJ4f5T9-UmU"
      },
      "outputs": [],
      "source": [
        "run.finish()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "HRw6Jx97SoLX"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
